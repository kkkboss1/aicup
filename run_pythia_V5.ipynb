{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm","authorship_tag":"ABX9TyOte5/pG/0vlvSw9gmkJ+1x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Hlkws9SDBKt","executionInfo":{"status":"ok","timestamp":1701497676964,"user_tz":-480,"elapsed":2176,"user":{"displayName":"趙叡","userId":"03186211742262374926"}},"outputId":"cce24ba2-1a64-4116-f92a-c4b5340c3d4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/mount\")"]},{"cell_type":"code","source":["! pip install torch > /dev/null\n","! pip install transformers > /dev/null\n","! pip install datasets > /dev/null"],"metadata":{"id":"uIXoCocJG7Qg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","# load model\n","# tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/mount/model\")\n","# model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/mount/model\").to('cuda')\n","\n","model_name = 'model_v5_EP8_2023_1130_1450_10'\n","\n","tokenizer = AutoTokenizer.from_pretrained(f\"/content/drive/MyDrive/Colab Notebooks/mount/V5/{model_name}\")\n","model = AutoModelForCausalLM.from_pretrained(f\"/content/drive/MyDrive/Colab Notebooks/mount/V5/{model_name}\").to('cuda')\n","\n","PAD_IDX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n","IGNORED_PAD_IDX = -100\n","PAD_IDX\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LZ1pt3a6DJUh","executionInfo":{"status":"ok","timestamp":1701497697287,"user_tz":-480,"elapsed":5146,"user":{"displayName":"趙叡","userId":"03186211742262374926"}},"outputId":"12f40c4f-aa68-43ce-8310-c4544d03a2ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"execute_result","data":{"text/plain":["50278"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["from datasets import load_dataset, Features, Value\n","valid_data = load_dataset(\n","          \"csv\", data_files=\"opendid_test.tsv\",\n","          delimiter='\\t',\n","          features = Features({'fid': Value('string'), 'idx': Value('int64'),'content': Value('string'), 'label': Value('string')}),\n","          column_names=['fid', 'idx', 'content', 'label']\n","        )\n","\n","valid_list= list(valid_data['train'])\n","valid_list[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKZ72I_UD00w","executionInfo":{"status":"ok","timestamp":1701497701234,"user_tz":-480,"elapsed":3952,"user":{"displayName":"趙叡","userId":"03186211742262374926"}},"outputId":"bcb08e7f-b158-419e-89b5-3ffabc4895e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'fid': '1097', 'idx': 1, 'content': '433475.RDC', 'label': None},\n"," {'fid': '1097', 'idx': 12, 'content': 'Timmins, ELDEN', 'label': None}]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from tqdm import tqdm#, tqdm_notebook\n","import torch\n","import datasets\n","bos = '<|endoftext|>'\n","eos = '<|END|>'\n","pad = '<|pad|>'\n","sep ='\\n\\n####\\n\\n'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","tokenizer.padding_side = 'left'\n","def sample_batch(model, tokenizer, input):\n","    \"\"\"Generate text from a trained model.\"\"\"\n","    model.eval()\n","    seeds = [f\"{bos} {text['content']} {sep}\" for text in input]\n","    texts = tokenizer(seeds, return_tensors = 'pt', padding=True).to(device)\n","    outputs = []\n","    #return\n","    with torch.cuda.amp.autocast():\n","        output_tokens = model.generate(**texts, max_new_tokens=400, pad_token_id = PAD_IDX,\n","                                        eos_token_id=tokenizer.convert_tokens_to_ids(eos))\n","        preds = tokenizer.batch_decode(output_tokens)\n","        for idx , pred in enumerate(preds):\n","            pred = pred[pred.index(sep)+len(sep):].replace(pad, \"\").replace(eos, \"\").strip()\n","            if pred == \"PHI: NULL\":\n","                continue\n","            phis = pred.split('\\n')\n","            lidxs = {}\n","            for p in phis:\n","                tid = p.find(':')\n","                if tid > 0:\n","                    text = p[tid+1:].strip()\n","                    nv = text.find('=>')\n","                    normalizedV = None\n","\n","                    if nv>0:\n","                      normalizedV = text[nv+2:]\n","                      text = text[:nv]\n","\n","                    lidx = 0\n","                    if text in lidxs:\n","                        lidx = lidxs[text]\n","                    lidx = input[idx]['content'].find(text, lidx)\n","                    eidx = lidx+len(text)\n","                    lidxs[text] = eidx\n","                    sidx=int(input[idx]['idx'])\n","                    if len(text) > 0 and p[:tid] in ['DOCTOR','DATE','IDNUM','MEDICALRECORD','PATIENT','HOSPITAL','TIME','DEPARTMENT','CITY','ZIP','STREET','STATE','AGE','ORGANIZATION','DURATION','PHONE','URL','LOCATION-OTHER','SET','COUNTRY','ROOM']:\n","                      if normalizedV is None:\n","                          outputs.append(f'{input[idx][\"fid\"]}\\t{p[:tid]}\\t{lidx+sidx}\\t{eidx+sidx}\\t{text}')\n","                      else:\n","                          outputs.append(f'{input[idx][\"fid\"]}\\t{p[:tid]}\\t{lidx+sidx}\\t{eidx+sidx}\\t{text}\\t{normalizedV}')\n","    return outputs\n","# utf8\n","# len(valid_list)\n","f = open(\"answer.txt\", \"w\" , encoding='utf-8')\n","# BATCH_SIZE = 400 # A100\n","BATCH_SIZE = 100 # V100\n","# BATCH_SIZE = 100  #T4\n","for i in tqdm(range(0, len(valid_list) , BATCH_SIZE)):\n","    with torch.no_grad():\n","        seeds = valid_list[i:i+BATCH_SIZE]\n","        outputs = sample_batch(model, tokenizer, input=seeds)\n","        for o in outputs:\n","            f.write(o)\n","            f.write('\\n')\n","f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mZGJeziGmJs","executionInfo":{"status":"ok","timestamp":1701498265914,"user_tz":-480,"elapsed":564684,"user":{"displayName":"趙叡","userId":"03186211742262374926"}},"outputId":"67d6dd66-8e1e-4807-836f-151003ce9dff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 790/790 [09:24<00:00,  1.40it/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mq7r1fmXG3az"},"execution_count":null,"outputs":[]}]}